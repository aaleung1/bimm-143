---
title: "Class 07: Machine Learning 1"
author: "Aaron (PID: A17544470)"
format: pdf
---

Today we will explore unsupervised machine learning methods including clustering and dimensionality reduction methods.

Let's start by making up some data (where we know that there are clear groups) that we can use to test out different clustering methods.

We can use the `rnorm()` function to help us here:

```{r}
hist(rnorm(n=100, mean = 10))
```

Make data `z` with two "clusters".
```{r}

x <- c(rnorm(30, mean = -3),
  rnorm(30, mean = 3)
)

z <- cbind(x=x,y=rev(x))

head(z)
plot(z)

```

How big is `z`
```{r}
nrow(z)
ncol(z)
```


## K-means clustering

The main function in "base" in R for K-means clustering is called `kmeans()`


```{r}
k <- kmeans(z, centers = 2)
k
```
finding what are the components of something
```{r}
attributes(k)
```


>Q. How many points lie in each cluster?

```{r}
k$size
```


>Q. What component of our results tells us about the cluster membership (i.e. which point lies in which cluster)?

```{r}
k$cluster
```

>Q. Center of each cluster?

```{r}
k$centers
```

>Q. Put this result info together and make a little "base R" plot of our clustering result. Also add the cluster center points to this plot.



```{r}
plot(z, col=c("blue","red"))
```

You can also color by number, such as (1,2,3...)

```{r}
plot(z, col = c(1,2))
```


You can also plot by cluster membership

```{r}
plot(z, col = k$cluster)
points(k$centers, col = "blue", pch = 17)
```


>Q. Run kmeans on our input `z` and define 4 clusters, making the same result visualization plot as above (plot of ze colored by cluster membership).

```{r}
k4 <- kmeans(z, centers = 4)

plot(z, col = k4$cluster)
points(k4$centers, col = "blue", pch = 17)
```

```{r}
k4$totss
k4$tot.withinss
```


## Hierarchical Clustering

The main function in base R for this is called `hclust()` it will take an input a distance matrix(key point is that you can't just give your "raw" data as input - you first have to calculate a distance matrix from your data).

This calculates every distance between each point to another.
```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h=8, col = "red")
```
Once I inspect the "tree" I can "cut" the tree to yield my groupings or clusters. The function to do this is called `cutree()`

```{r}
groups <- cutree(hc, h = 8)
```


```{r}
plot(z, col = groups)
```

## Hands on with Principal Component Analysis (PCA)

Let's examine a 17-dimensional data detailing food consumption in the UK(England, Scotland, N. Ireland, Wales)

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
x
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
dim(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The x <- read.csv(url, row.names=1) approach is the one I prefer more because if it is ran multiple times it may return an error with an incorrect number of dimensions


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

>Q3. Changing what optional argument in the above barplot() function results in the following plot?

Changing the beside argument to be false results in the following plot.
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```


>Q5. Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col = rainbow(10), pch = 16)
```

Looking at these types of "pairwise plots" can be helpful but does not scale well and kind of sucks(time consuming and error prone)! There must be a better way...

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

N. Ireland is more different compared to the other countries of UK as it varies more than other countries.


### PCA to the rescue

The main function for PCA in base R is called `prcomp()`. This function wants the transpose of our input data - i.e. important food categories as column titles and the countries as rows.


```{r}
pca <- prcomp(t(x)) 
summary(pca)
```
Proportion of variance shows how much of the action is being done in that principal component.

Let's see what is in our PCA object `pca`
```{r}
attributes(pca)
```

The `pca$x` result object is where we will focus first as this details how the countries are related to each other in terms of our new "axis" (aka "PCs", "eigenvectors", etc.)

```{r}
head(pca$x)
```

>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
coloring <- c("orange","red", "blue", "darkgreen")
text(pca$x[,1], pca$x[,2], colnames(x), col = coloring)
```

>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col = c("orange","red", "blue", "darkgreen"))
```

We can look at the so called PC "loadings" result object to see how the original foods contribute to our new PCs (i.e. how the original variables contribute to our new better PC variables).

```{r}
pca$rotation[,1]
```
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```


>Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?


```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

# PCA of RNA-seq data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
dim(rna.data)
```

>Q10 How many genes and samples are in this data set?

There are 100 genes and 10 samples.













